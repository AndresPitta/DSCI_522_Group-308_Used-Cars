---
title: "Predicting Used Car Prices"
author: "Andr√©s Pitta, Braden Tam, Serhiy Pokrovskyy </br>"
date: "2020/01/25 (updated: `r Sys.Date()`)"
always_allow_html: yes
output: 
  github_document:
    toc: true
    pandoc_args: --webtex
bibliography: used_cars.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(kableExtra)
library(tidyverse)
```

```{r load data, echo=FALSE, message=FALSE, warning=FALSE}
df <- read_csv("../results/test_results_sample.csv") %>% 
  dplyr::select(-c(X1, abs_error))
scores <- read_csv("../results/train_metrics.csv")

scores_text <- scores %>%
  dplyr::rename(train = 'Train R^2',
                test = 'Test R^2')
```



# Summary

In this project we attempt to build a regression model which can predict the price of used cars based on numerous features of the car. We tested the following models: support vector regression, stochastic gradient descent regression, linear regression, K-nearest neighbour regression, and random forest regression.  We found that support vector regression had the best results, having an $R^2$ score of `r round(scores_text$train[nrow(scores)],3)` on the training set and an $R^2$ score of `r round(scores_text$test[nrow(scores)],3)` on the test set. Given that the dataset was imbalanced, this led to poor prediction of the classes that were quite sparse because the model was not able to learn enough about those classes in order to give good predictions on unseen data. 


# Introduction

Websites such as Craigslist, Kijiji, and eBay have tons of users that create a wide array of used good markets. Typically people looking to save some money use these website to purchase second hand items. The problem with these websites is that the user determines the price of their used good. This can either be a good or bad thing, depending on whether or not the user is trying to scam the buyer or give the buyer a good deal. For the average individual who is not familiar with prices of the used market, it is especially difficult to gauge what the price of a used good should be. Being able to predict used car prices based on data on a whole market will gives users the ability to evaluate whether a used car listing is consistent with the market so that they know they are not getting ripped off. 

# Methods

## Data
The data set used in this project is Used Cars Dataset created by Austin Reese. It was collected from Kaggle.com [@reese_2020] and can be found [here](https://www.kaggle.com/austinreese/craigslist-carstrucks-data). This data consists of used car listings in the US scraped from Craigslist that contains information such as listed price, manufacturer, model, listed condition, fuel type, odometer, type of car, and which state it's being sold in. 

## Analysis

The R and Python programming languages [@R; @Python] and the following R and Python packages were used to perform the analysis: docopt [@docopt], knitr [@knitr], tidyverse [@tidyverse], readr [@readr] docopt [@docoptpython], altair [@Altair2018], plotly [@plotly], selenium [@seleniumhq_2020], pandas [@mckinney-proc-scipy-2010], numpy [@oliphant2006guide], statsmodel [@seabold2010statsmodels]. scikit-learn [@sklearn_api]. 

The code used to perform the analysis and create this report can be found [here](https://github.com/UBC-MDS/DSCI_522_Group-308_Used-Cars)

  
As it was mentioned, our original data holds half a million observations with a few dozen features, most categorical, so accurate feature selection and model selection were extremely important. Especially because model training took significant amount of computational resources.

Since we could not efficiently use automated feature selection like RFE or FFE (because of time / resources constraint), we had to perform manual feature selection. As we had some intuition in the target area as well as some practical experience, we were able to prune our feature list to just 12 most important on our opinion:

- 10 categorical features:
    - manufacturer (brand)
    - transmission type
    - fuel type
    - paint color
    - number of cylinders
    - drive type (AWD / FWD / RWD)
    - size
    - condition
    - title_status
    - state

- 2 continuous features:
    - year
    - odometer
    
The following plots are just a few examples of us visual representations of what variables seem to be important in predicting used car prices. The code used to generate these plots can be found [here](https://github.com/UBC-MDS/DSCI_522_Group-308_Used-Cars/blob/master/scripts/eda.py).

```{r, echo=FALSE, out.width = '80%'}
path <- "../results/figures/manufacturer.png"
knitr::include_graphics(path)
```

```{r, echo=FALSE, out.width = '80%'}
path <- "../results/figures/map_price.png"
knitr::include_graphics(path)
```

```{r, echo=FALSE, out.width = '80%'}
path <- "../results/figures/corrplot.png"
knitr::include_graphics(path)
```
 
 
For hyper-paramter tuning of each model we performed a 5-fold-cross-validated grid search involving a range of the most important model-specific hyper-parameters. We chose to use 5-folds because we have a lot of data to work with so this amount would provide an optimal trade-off between computational time and finding the most unbiased estimates of our models.

# Results & Discussion


Based on our EDA and assumptions, we picked a number of models to fit our train data. Since training and validating took a lot of resources, we performed it on a gradually increasing subsets of training data in the hopes that we find an optimal amount of required data for maximal performance. See the results below, sorted by validation score (increasing):


```{r table scores, echo=FALSE, message=FALSE}
kableExtra::kable_styling(knitr::kable(scores))
```


Since SVM shown the best results from the very beginning, we performed a thorough adaptive grid search on more training data (200,000 observations, running for 4 hours) resulting in 81.4% accuracy on validation data. Eventually we ran the model on the **test data** containing more than 40,000 observations, which confirmed the model with even better **accuracy of 81.6%**. The good sign was also that it did not overfit greatly on train set, which was a good sign to perform further testing. 

Metric | Value
--- | ---
Accuracy | 0.815724
RMSE | 4366.43
MAE | 2691.71
Average Price | 13819.99

Here is a list of test examples showing the predicted used car prices:

```{r table examples, echo=FALSE, message=FALSE}
kableExtra::kable_styling(knitr::kable(df))
```


# Further Directions

To further imrpove the accuracy of this model we can aleviate the problem of imbalanced classes by grouping manufacturers by region (American, Germnan, Italian, Japanese, British, etc.) and status type (luxery vs economy). 

Although we achieved a nice accuracy of 81.5%, we can now observe some other metrics. Eg., having an RMSE almost twice higher than MAE suggests that there is a good number of observations where the error is big (the more RMSE differs from MAE, the higher is the variance) This is something we may want to improve by finding features and clusters in data space that introduce more variance in the predictions. Eg. the model predicting clean car price may greatly differ from the model predicting salvage (damage / total loss) car price. This comes from getting deeper expertise in the area, and we will try to play with this further more.

We may also want to use a different scoring function for our model - eg. some custom implementation of MSE of relative error, since we have high variance of price in the original dataset.

Lastly, due to time / resources limitations we only trained the model on half the training data, so we should try to run it on all training data and see how this changes our model (this would take approximately 16 hours). So far we have only seen improvements to the score as we increased the sample size.

The ultimate end goal is to eventually create a command-line tool for the end-user to interactively request vehicle details and output expected price with a precision interval.


# References
