---
title: "Predicting Used Car Prices"
author: "Andr√©s Pitta, Braden Tam, Serhiy Pokrovskyy </br>"
date: "2020/01/25 (updated: `r Sys.Date()`)"
always_allow_html: true
output: 
  html_document:
    toc: true
bibliography: used_cars.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
library(kableExtra)
library(tidyverse)
library(caret)
```

```{r load model results}
# model <- readRDS("../results/final_model.rds")
# model_quality <- readRDS("../results/final_model_quality.rds")
```

# Summary

In this project we attempt to build a regression model which can predict the price of used cars based on numerous features of the car. We tested the following models: support vector regression, stochastic gradient descent regression, linear regression, K-nearest neighbour regression, and random forest regression. data set is not linearly separable, more clustered. We found that support vector regression had the best results, having a score of `INPUT TRAIN SCORE` on the training set and a score of `INPUT TEST SCORE` on the test set. Given that the dataset was imbalanced, this led to poor prediction of the classes that were quite sparse. 

# Introduction

Websites such as Craigslist, Kijiji, and eBay have tons of users that creates vast markets of used goods. Typically people looking to save some money use these website to purchase second hand items. The problem with these websites is that the user determines the price of their used good. This can either be a good or bad thing, depending on whether or not the user is trying to scam the buyer or give the buyer a good deal. For the average individual who is not familiar with prices of the used market, it is especially difficult to guage what the price of a used good should be. Being able to predict used car prices based on data on a whole market will gives users the ability to evaluate whether a used car listing is consistent with the market so that they know they are not getting ripped off. 

# Methods

## Data
The data set used in this project is Used Cars Dataset created by Austin Reese. It was collected from Kaggle.com [@reese_2020] and can be found [here](https://www.kaggle.com/austinreese/craigslist-carstrucks-data). This data consists of used car listings in the US scraped from Craigslist that contains information such as listed price, manufacturer, model, listed condition, fuel type, odometer, type of car, and which state it's being sold in. 

## Analysis
As it was mentioned, our original data holds half a million observations with a few dozen features, most categorical, so accurate feature selection and model selection were extremely important. Especially because model training took significant amount of computational resources.

Since we could not efficiently use automated feature selection like RFE or FFE (because of time / resources constraint), we had to perform manual feature selection. As we had some intuition in the target area as well as some practical experience, we were able to prune our feature list to just 12 most important on our opinion:

10 categorical features:
- manufacturer (brand)
- transmission type
- fuel type
- paint color
- number of cylinders
- drive type (AWD / FWD / RWD)
- size
- condition
- title_status
- state

2 continuous features:
- year
- odometer

For each model we performed 5-fold-cross-validated grid search involving a range of most important model-specific hyper-parameters.

The R and Python programming languages [@R; @Python] and the following R and Python packages were used to perform the analysis: docopt [@docopt], knitr [@knitr], tidyverse [@tidyverse], readr [@readr] docopt [@docoptpython], altair [@Altair2018], plotly [@plotly], selenium [@seleniumhq_2020], pandas [@mckinney-proc-scipy-2010], numpy [@oliphant2006guide], statsmodel [@seabold2010statsmodels]. scikit-learn [@sklearn_api]. 


## Dependencies
- Python 3.7.3 and Python packages:
    - altair==3.2.0
    - selenium==3.141.0
    - docopt==0.6.2
    - pandas==0.24.2
    - numpy==1.16.4
    - statsmodel==0.10.0
    - plotly==4.3.0
    - scikit-learn==0.20.4
- R version 3.6.1 and R packages:
    - knitr==1.24
    - docopt==0.6.1
    - tidyverse==1.3.0
    - readr==1.3.1
    
The code used to perform the analysis and create this report can be found here: *https://github.com/ttimbers/breast_cancer_predictor.*******

# Results & Discussion

Based on our EDA and assumptions, we picked a number of models to fit our train data. Since training and validating took a lot of resources, we performed it on a gradually increasing subsets of training data. See the results below, sorted by validation score (increasing):

Model | Training Score | Validation Score
--- | --- | ---
Linear Regression | 0.555803 | 0.526354
Stochastic Gradient Descent | 0.550439 | 0.528612
kNN | 0.638008 | 0.626848
Random Forests | 0.964447 | 0.734342
Gradient Boosted Trees | 0.803595 | 0.736818
**Support Vector Machines** | **0.840271** | **0.813724**

Since SVM shown the best results from the very beginning, we performed a thorough adaptive grid search on a bigger subset of 200,000 observations (running for 4 hours) resulting in 81.3% accuracy on validation data. Eventually we ran the model on the **test data** containing more than 40,000 observations, which confirmed the model with even better **accuracy of 81.5%**. The good sign was also that it did not overfit greatly on train set, which was a good sign to perform further testing. 


```{r predictor-distributions, echo=FALSE, fig.cap="Figure 1. Comparison of the empirical distributions of training data predictors between benign and malignant tumour masses.", out.width = '100%'}
knitr::include_graphics("../results/predictor_distributions_across_class.png")
```



```{r choosing-k, echo=FALSE, fig.cap="Figure 2. Results from 30-fold cross validation to choose K. Cohen's Kappa was used as the classification metric as K was varied.", out.width = '60%'}
knitr::include_graphics("../results/kappa_vs_k.png")
```

Our prediction model performed quite well on test data, with a final Cohen's Kappa score of `r round(model_quality$overall[2], 1)` and an overall accuracy calculated to be `r round(model_quality$overall[1], 2)`. Other indicators that our model performed well come from the confusion matrix, where it only made `r model_quality$table[2, 1] + model_quality$table[1, 2]` mistakes. However all `r model_quality$table[2, 1] + model_quality$table[1, 2]` mistakes were predicting a malignant tumour as benign, given the implications this has for patients health, this model is not good enough to yet implement in the clinic.


```{r confusion-matrix, echo=FALSE}
kable(model_quality$table, caption = "Table 1. Confusion matrix of model performance on test data.") %>%
  kable_styling(full_width = FALSE) %>%
  add_header_above(c(" ", "Reference" = 2)) %>% 
  pack_rows("Predicted", 1, 2)
```

# Further Directions

To further imrpove the accuracy of this model we can aleviate the problem of imbalanced classes by grouping manufacturers by region (American, Germnan, Italian, Japanese, British, etc.) and status type (luxery vs economy). 

Although we achieved a nice accuracy of 81.5%, we can now observe some other metrics. Eg., having an RMSE almost twice higher than MAE suggests that there is a good number of observations where the error is big (the more RMSE differs from MAE, the higher is the variance) This is something we may want to improve by finding features and clusters in data space that introduce more variance in the predictions. Eg. the model predicting clean car price may greatly differ from the model predicting salvage (damage / total loss) car price. This comes from getting deeper expertise in the area, and we will try to play with this further more.

We may also want to use a different scoring function for our model - eg. some custom implementation of MSE of relative error, since we have high variance of price in the original dataset.

Lastly, due to time / resources limitations we only trained the model on half the training data, so we should try to run it on all training data and see how this changes our model (this would take approximately 16 hours). So far we have only seen improvements to the score as we increased the sample size.

The ultimate end goal is to eventually create a command-line tool for the end-user to interactively request vehicle details and output expected price with a precision interval.


# References
