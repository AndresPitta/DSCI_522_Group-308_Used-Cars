---
title: "Predicting Used Car Prices"
author: "Andr√©s Pitta, Braden Tam, Serhiy Pokrovskyy </br>"
date: "2020/01/25 (updated: `r Sys.Date()`)"
always_allow_html: yes
output: 
  github_document:
    toc: true
    pandoc_args: --webtex
bibliography: used_cars.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(kableExtra)
library(tidyverse)
library(knitr)
```

```{r load data, echo=FALSE, message=FALSE, warning=FALSE}
df <- read_csv("../results/test_results_sample.csv") %>% 
  dplyr::select(-c(X1, abs_error, title_status)) %>%
  rename(Year = year,
         Odometer = odometer,
         Manufacturer = manufacturer,
         Condition = condition, 
         'Price (USD)' = price,
         Prediction = prediction,
         'Absolute Error (%)' = abs_error_pct)

train_scores <- read_csv("../results/train_metrics.csv") %>%
  dplyr::rename(Model = X1) %>%
  mutate(`Train Score` = round(`Train Score`, 3),
         `Validation Score` = round(`Validation Score`, 3))

train_scores_text <- train_scores %>%
  dplyr::rename(train = 'Train Score',
                valid = 'Validation Score')

test_scores <- read_csv("../results/test_metrics.csv")
names(test_scores) <- c('Metric', 'Value')
```

# Summary

In this project we attempt to build a regression model which can predict the price of used cars based on numerous features of the car. We tested the following models: support vector machine regression, stochastic gradient descent regression, linear regression, K-nearest neighbour regression as well as random forest regression and gradient boosted trees.  We found that support vector machine regression shown the best results, producing an $R^2$ score of `r round(train_scores_text$train[1],3)` on the training set, `r round(train_scores_text$valid[1],3)` on the validation set and `r round(test_scores$Value[1], 3)` on the test set. The training and validation scores are computed from a very small subset of the data while the test score used a much larger subset. Given that the dataset was imbalanced by manufacturers, this led to a bit worse prediction of the classes that were quite sparse because the model was not able to learn enough about those classes in order to give good predictions on unseen data.

# Introduction

Websites such as Craigslist, Kijiji, and eBay have tons of users that create a wide array of used good markets. Typically people looking to save some money use these website to purchase second hand items. The problem with these websites is that the user determines the price of their used good. This can either be a good or bad thing, depending on whether or not the user is trying to scam the buyer or give the buyer a good deal. For the average individual who is not familiar with prices of the used market, it is especially difficult to gauge what the price of a used good should be. Being able to predict used car prices based on data on a whole market will gives users the ability to evaluate whether a used car listing is consistent with the market so that they know they are not getting ripped off. 

# Methods

## Data
The data set used in this project is "Used Cars Dataset" created by Austin Reese. It was collected from Kaggle.com [@reese_2020] and can be found [here](https://www.kaggle.com/austinreese/craigslist-carstrucks-data). This data consists of used car listings in the US scraped from Craigslist that contains information such as listed price, manufacturer, model, listed condition, fuel type, odometer, type of car, and which state it's being sold in.

## Analysis

The R and Python programming languages [@R; @Python] and the following R and Python packages were used to perform the analysis: docopt [@docopt], knitr [@knitr], tidyverse [@tidyverse], readr [@readr] docopt [@docoptpython], altair [@Altair2018], plotly [@plotly], selenium [@seleniumhq_2020], pandas [@mckinney-proc-scipy-2010], numpy [@oliphant2006guide], statsmodel [@seabold2010statsmodels]. scikit-learn [@sklearn_api]. 

The code used to perform the analysis and create this report can be found [here](https://github.com/UBC-MDS/DSCI_522_Group-308_Used-Cars)

As it was mentioned, our original data holds half a million observations with a few dozen features, most categorical, so accurate feature selection and model selection were extremely important. Especially because model training took significant amount of computational resources.

Since we could not efficiently use automated feature selection like RFE or FFE (because of time / resources constraint), we had to perform manual feature selection. As we had some intuition in the target area as well as some practical experience, we were able to prune our feature list to just 12 most important on our opinion:

- 10 categorical features:
    - manufacturer (brand)
    - transmission type
    - fuel type
    - paint color
    - number of cylinders
    - drive type (AWD / FWD / RWD)
    - size
    - condition
    - title_status
    - state

- 2 continuous features:
    - year
    - odometer
    
The following plots are just a few examples of us visual representations of what variables seem to be important in predicting used car prices. The code used to generate these plots can be found [here](https://github.com/UBC-MDS/DSCI_522_Group-308_Used-Cars/blob/master/scripts/eda.py).


```{r, echo=FALSE, out.width = '44%'}
path <- "../results/figures/manufacturer.png"
knitr::include_graphics(path)
```

```{r, echo=FALSE, out.width = '53%'}
path <- "../results/figures/map_price.png"
knitr::include_graphics(path)
```

 
For hyper-paramter tuning of each model we performed a 5-fold-cross-validated grid search involving a range of the most important model-specific hyper-parameters. We chose to use 5-folds because we have a lot of data to work with so this amount would provide an optimal trade-off between computational time and finding the most unbiased estimates of our models.

# Results & Discussion


Based on our EDA and assumptions, we picked a number of models to fit our train data. Since training and validating took a lot of resources, we performed it on a gradually increasing subsets of training data in the hopes that we find an optimal amount of required data for maximal performance. The metric used to evaluate our model is $R^2$, which is a value from 0 to 1 that gives the proportions of the variance in price that is explained by our model. See the results below, sorted by validation score:


```{r table scores, echo=FALSE, message=FALSE}
train_scores %>%
  knitr::kable("html") %>%
  kableExtra::kable_styling(position = "center", full_width = F)
```


Since SVM shown the best results from the very beginning, we performed a thorough adaptive grid search on more training data (200,000 observations, running for 4 hours) to devise a more robust model. Finally, we ran the model on the **test data** containing more than 40,000 observations, which confirmed the model with an $R^2$ value of **0.816**. The good sign was also that it did not overfit greatly on train set, which was a good sign to perform further testing.

Later, we ran model testing on full training dataset taking around 12 hours, and got similar but even better results:

Metric | Value
--- | ---
$R^2$ | `r round(test_scores$Value[1], 3)`
RMSE | `r round(test_scores$Value[2], 2)`
MAE | `r round(test_scores$Value[3], 2)`

Here is a list of test examples showing the predicted used car prices:

```{r table examples, echo=FALSE, message=FALSE}
kableExtra::kable_styling(knitr::kable(df), position = "center", full_width = F)
```


# Further Directions

To further imrpove the $R^2$ of this model we can aleviate the problem of imbalanced classes by grouping manufacturers by region (American, Germnan, Italian, Japanese, British, etc.) and status type (luxery vs economy). 

Although we achieved a solid $R^2$ value of `r round(test_scores$Value[1], 3)`, we can now observe some other metrics. Eg., having an RMSE (`r round(test_scores$Value[2], 2)`) almost twice higher than MAE (`r round(test_scores$Value[3], 2)`) suggests that there is a good number of observations where the error is big (the more RMSE differs from MAE, the higher is the variance). This is something we may want to improve by finding features and clusters in data space that introduce more variance in the predictions. Eg. the model predicting clean car price may greatly differ from the model predicting salvage (damage / total loss) car price. This comes from getting deeper expertise in the area, and we will try to play with this further more.

We may also want to use a different scoring function for our model - eg. some custom implementation of MSE of relative error, since we have high variance of price in the original dataset.

Previously, we ran it on half of the training data which took us around 4 hours to train and resulted in 0.816 score. We were now able to run it on a full training dataset (taking approximately 12 hours), which improved the score by `r round(test_scores$Value[1] - 0.816, 3)` (not much for such a big increase in training time, but still an improvement)

The ultimate end goal is to eventually create a command-line tool for the end-user to interactively request vehicle details and output expected price with a confidence interval.

# References
